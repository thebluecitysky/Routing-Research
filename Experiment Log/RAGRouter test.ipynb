{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd8f0f1-5d4e-4e0f-a3bf-0dd8483195ef",
   "metadata": {},
   "source": [
    "# 1.Data\n",
    "## 生成依存樹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a80da96f-d0dd-4711-a2b1-1db4081c0c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: /root/autodl-tmp/RouteLLM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# 获取当前 notebook 所在目录（适用于 Jupyter）\n",
    "notebook_dir = Path(os.getcwd())\n",
    "print(\"Notebook directory:\", notebook_dir)\n",
    "\n",
    "# 添加到 sys.path\n",
    "sys.path.append(str(notebook_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf8493d-4e0c-47a3-9eae-74723d9334e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import dgl\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def build_dependency_graph(text, graph_id, save_dir=\"./saved/graph\"):\n",
    "    \"\"\"构建依存语法图并保存\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    edges = []\n",
    "    for token in doc:\n",
    "        if token.head.i != token.i:  # 排除自环\n",
    "            edges.append((token.head.i, token.i))\n",
    "\n",
    "    # 创建DGL图\n",
    "    src_nodes = [s for s, d in edges]\n",
    "    dst_nodes = [d for s, d in edges]\n",
    "    g = dgl.graph((src_nodes, dst_nodes))\n",
    "    \n",
    "    # 为每个节点添加特征（假设特征维度为300）\n",
    "    num_nodes = g.num_nodes()\n",
    "    node_features = torch.randn(num_nodes, 300)  # 随机初始化节点特征\n",
    "    g.ndata['feat'] = node_features  # 将特征存储在 'feat' 字段中\n",
    "\n",
    "    # 保存图\n",
    "    dgl.save_graphs(os.path.join(save_dir, f\"{graph_id}.dgl\"), [g])\n",
    "\n",
    "# 示例使用\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 读取 combined_data.csv\n",
    "    df = pd.read_csv('combined_data.csv')\n",
    "\n",
    "    # 使用 tqdm 遍历每一行的 prompt 并调用 build_dependency_graph\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing prompts\"):\n",
    "        prompt = row['prompt']\n",
    "        graph_id = f\"graph_{idx}\"  # 使用索引作为图的唯一 ID\n",
    "        build_dependency_graph(prompt, graph_id)\n",
    "\n",
    "    print(\"All prompts processed and graphs saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd1ab07-6ec6-48f2-8fdf-b359ced6f713",
   "metadata": {},
   "source": [
    "## ConceptNet数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4355f76d-c27e-4b26-8d98-49e0d4a1f92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_ENDPOINT=https://hf-mirror.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf3dcd6-741d-4d21-bad3-575639d50d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from src.DataAugmentation.run_deepseek32b import deepseek_generate\n",
    "\n",
    "def routing_decision(score):\n",
    "    \"\"\"根据评分进行路由决策\"\"\"\n",
    "    return \"strong_model\" if score < 4 else \"weak_model\"\n",
    "\n",
    "# 初始化NLP模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_entities(prompt):\n",
    "    \"\"\"使用spacy提取核心实体\"\"\"\n",
    "    doc = nlp(prompt)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ in ['GPE', 'LOC', 'ORG', 'PERSON']]\n",
    "\n",
    "import requests\n",
    "\n",
    "def query_conceptnet(entity):\n",
    "    \"\"\"查询ConceptNet获取关联实体\"\"\"\n",
    "    url = f\"http://api.conceptnet.io/query?node=/c/en/{entity}&limit=10\"\n",
    "    response = requests.get(url).json()\n",
    "    \n",
    "    related = []\n",
    "    for edge in response['edges']:\n",
    "        end = edge['end']['label'].lower()\n",
    "        if end != entity.lower():\n",
    "            related.append((end, edge['rel']['label']))\n",
    "    return list(set(related))[:5]  # 取前5个不重复关联\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "def enhance_prompt(original, entities):\n",
    "    \"\"\"构建增强查询模板\"\"\"\n",
    "    return f\"{original} 请结合以下概念进行详细解释：{', '.join(entities)}\"\n",
    "\n",
    "def generate_response(prompt):\n",
    "    \"\"\"使用Mixtral生成回答\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=512)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def knowledge_graph_augmentation(df, threshold=3):\n",
    "\n",
    "    instruct_prompt = \"\"\"\n",
    "    You are tasked with evaluating the quality of responses generated by two models—a smaller model and a larger model—for a given question \\( q \\). \n",
    "    The smaller model's response is \\( \\text{response\\_a} \\), and the larger model's response is \\( \\text{response\\_b} \\). \n",
    "    Based on the quality of the responses, assign a score between 1 and 5, where:\n",
    "    - **1-3**: The response quality is not good enough, and the question is better suited for the smaller (weaker) model.\n",
    "    - **4-5**: The response quality is good, and the question is better suited for the larger (stronger) model.\n",
    "    Your task is to determine whether the question \\( q \\) is better suited for the smaller model or the larger model based on the response quality. Only generate a single numerical score between 1 and 5. Do not provide any additional explanation or context.\n",
    "    **Output Format:**  \n",
    "    A single integer between 1 and 5.  \n",
    "    **Example:**  \n",
    "    If the response quality is good and the question is better suited for the larger model, output:  \n",
    "    `5`  \n",
    "    If the response quality is not good enough and the question is better suited for the smaller model, output:  \n",
    "    `2`  \n",
    "    **Your Output:**  \n",
    "    `[Your score here]`\n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    expanded_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        # 实体提取\n",
    "        entities = extract_entities(row['prompt'])\n",
    "        if not entities:\n",
    "            continue\n",
    "            \n",
    "        # 知识图谱查询\n",
    "        all_related = []\n",
    "        for ent in entities:\n",
    "            all_related += [r[0] for r in query_conceptnet(ent)]\n",
    "        \n",
    "        # 生成增强提示\n",
    "        enhanced_prompt = enhance_prompt(row['prompt'], list(set(all_related)))\n",
    "        \n",
    "        # 小模型生成回答\n",
    "        small_llm_a = generate_response(enhanced_prompt)\n",
    "        # deepseek标注\n",
    "        large_llm_a = deepseek_generate(enhanced_prompt)\n",
    "\n",
    "        score = deepseek_generate(\n",
    "            instruct_prompt.replace('{response\\_a}', small_llm_a).replace('{response\\_b}', large_llm_a).replace('\\( q \\)', q_prime)\n",
    "            )\n",
    "        if int(score) > threshold:\n",
    "            winner = 'a'\n",
    "        elif int(score) == threshold:\n",
    "            winner == \"tie\"\n",
    "        else:\n",
    "            winner = 'b'\n",
    "\n",
    "        # 构造新数据行\n",
    "        new_row = {\n",
    "            \"id\": len(expanded_data) + 1,  # 生成唯一 ID\n",
    "            \"model_a\": 'Mixtral-8x7B',\n",
    "            \"model_b\": \"deepseek-32b\",\n",
    "            \"prompt\": enhanced_prompt,\n",
    "            \"response_a\": small_llm_a,\n",
    "            \"response_b\": large_llm_a,\n",
    "            \"winner_model_a\": 1 if winner == \"a\" else 0,\n",
    "            \"winner_model_b\": 1 if winner == \"b\" else 0,\n",
    "            \"winner_tie\": 1 if winner == \"tie\" else 0,\n",
    "            \"score\": int(score)\n",
    "        }\n",
    "        \n",
    "        # 添加到扩充数据集中\n",
    "        expanded_data.append(new_row)\n",
    "    \n",
    "    return pd.DataFrame(expanded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb397b5-aa50-4fa0-94d1-750791e677aa",
   "metadata": {},
   "source": [
    "## 采样增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a5b9b86-89bb-4988-a43a-9f28ac49efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import beta\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 1. 困惑度计算模块\n",
    "class PerplexityCalculator:\n",
    "    def __init__(self, model_name=\"gpt2\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def calculate_ppl(self, text, stride=512):\n",
    "        \"\"\"计算单个文本的困惑度\"\"\"\n",
    "        encodings = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        max_length = self.model.config.n_positions\n",
    "        seq_len = encodings.input_ids.size(1)\n",
    "        \n",
    "        nlls = []\n",
    "        for begin_index in range(0, seq_len, stride):\n",
    "            end_index = min(begin_index + max_length, seq_len)\n",
    "            input_ids = encodings.input_ids[:, begin_index:end_index].to(self.device)\n",
    "            target_ids = input_ids.clone()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids, labels=target_ids)\n",
    "                neg_log_likelihood = outputs.loss\n",
    "            \n",
    "            nlls.append(neg_log_likelihood)\n",
    "        \n",
    "        ppl = torch.exp(torch.stack(nlls).mean())\n",
    "        return ppl.item()\n",
    "\n",
    "# 2. 动态采样策略核心类\n",
    "class DynamicSampler:\n",
    "    def __init__(self, alpha=1.5, beta_params=(2,5)):\n",
    "        self.alpha = alpha          # 难度权重系数\n",
    "        self.beta_a, self.beta_b = beta_params  # Beta分布参数\n",
    "    \n",
    "    def _assign_difficulty_levels(self, ppl_series):\n",
    "        \"\"\"使用四分位数划分难度等级\"\"\"\n",
    "        quantiles = ppl_series.quantile([0.25, 0.5, 0.75])\n",
    "        bins = [-np.inf, quantiles[0.25], quantiles[0.5], quantiles[0.75], np.inf]\n",
    "        labels = ['L1', 'L2', 'L3', 'L4']\n",
    "        return pd.cut(ppl_series, bins=bins, labels=labels)\n",
    "    \n",
    "    def _calculate_sampling_weights(self, df):\n",
    "        \"\"\"计算归一化采样权重\"\"\"\n",
    "        weighted_ppl = np.power(df['ppl'], self.alpha)\n",
    "        total = weighted_ppl.sum()\n",
    "        return weighted_ppl / total\n",
    "    \n",
    "    def _beta_sampling_factor(self, difficulty_level):\n",
    "        \"\"\"根据难度等级生成Beta分布采样因子\"\"\"\n",
    "        level_map = {'L1':0.1, 'L2':0.3, 'L3':0.7, 'L4':0.9}  # 各等级基准值\n",
    "        base = level_map[difficulty_level]\n",
    "        return beta.ppf(base, self.beta_a, self.beta_b)\n",
    "    \n",
    "    def resample_data(self, df, target_size):\n",
    "        \"\"\"执行动态重采样\"\"\"\n",
    "        # 计算基础权重\n",
    "        df['base_weight'] = self._calculate_sampling_weights(df)\n",
    "        \n",
    "        # 应用Beta分布调整\n",
    "        df['beta_factor'] = df['difficulty_level'].apply(self._beta_sampling_factor)\n",
    "        df['final_weight'] = df['base_weight'] * df['beta_factor']\n",
    "        df['final_weight'] /= df['final_weight'].sum()  # 重新归一化\n",
    "        \n",
    "        # 分层采样\n",
    "        sampled_df = df.sample(\n",
    "            n=target_size,\n",
    "            weights='final_weight',\n",
    "            replace=True,   # 允许过采样\n",
    "            random_state=42\n",
    "        ).reset_index(drop=True)\n",
    "        \n",
    "        return sampled_df\n",
    "\n",
    "# 3. 完整处理流程\n",
    "def main_process(input_path, output_path, target_size=200000):\n",
    "    # 加载数据\n",
    "    df = pd.read_csv(input_path)\n",
    "    print(f\"原始数据量：{len(df)} 条\")\n",
    "    \n",
    "    # 步骤1：计算困惑度\n",
    "    ppl_calculator = PerplexityCalculator()\n",
    "    df['ppl'] = df['prompt'].progress_apply(ppl_calculator.calculate_ppl)  # 使用tqdm进度条\n",
    "    \n",
    "    # 步骤2：划分难度等级\n",
    "    sampler = DynamicSampler(alpha=1.5)\n",
    "    df['difficulty_level'] = sampler._assign_difficulty_levels(df['ppl'])\n",
    "    \n",
    "    # 步骤3：动态重采样\n",
    "    resampled_df = sampler.resample_data(df, target_size)\n",
    "    \n",
    "    # 保存结果\n",
    "    resampled_df.to_csv(output_path, index=False)\n",
    "    print(f\"采样后数据量：{len(resampled_df)} 条\")\n",
    "    \n",
    "    # 打印分布统计\n",
    "    dist = resampled_df['difficulty_level'].value_counts(normalize=True)\n",
    "    print(\"\\n难度分布：\")\n",
    "    print(dist.sort_index())\n",
    "\n",
    "# 示例执行\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"original_data.csv\"\n",
    "    output_file = \"resampled_data.csv\"\n",
    "    main_process(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71916358-a60b-46c2-adf1-3317c102fb3e",
   "metadata": {},
   "source": [
    "## RAG向量数据库搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61890ca2-7725-48e4-8980-f168cf7b94bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "data_df = pd.read_csv('combined_data.csv')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('autodl-tmp/RouteLLM/pretrained/bert')\n",
    "model = BertModel.from_pretrained('autodl-tmp/RouteLLM/pretrained/bert')\n",
    "\n",
    "def encode_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "def retrieve_documents(query, top_k=3):\n",
    "    query_vector = encode_text(query)\n",
    "    distances, indices = index.search(query_vector, top_k)\n",
    "    return [documents[i] for i in indices[0]]\n",
    "\n",
    "documents = data_df[[\"prompt\"]].tolist()\n",
    "doc_vectors = np.array([encode_text(doc) for doc in documents])\n",
    "\n",
    "index = faiss.IndexFlatL2(doc_vectors.shape[1])\n",
    "index.add(doc_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bab78d-8303-41f5-9b9f-283e996de7db",
   "metadata": {},
   "source": [
    "## 增强后数据展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6371c3d-93b9-4283-9853-0ad021f1e5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"What is the difference between marriage lice...</td>\n",
       "      <td>[\"A marriage license is a legal document that ...</td>\n",
       "      <td>[\"A marriage license and a marriage certificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>gpt-3.5-turbo-0613</td>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>[\"explain function calling. how would you call...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>[\"Function calling is the process of invoking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>mistral-7b-instruct</td>\n",
       "      <td>[\"How can I create a test set for a very rare ...</td>\n",
       "      <td>[\"Creating a test set for a very rare category...</td>\n",
       "      <td>[\"When building a classifier for a very rare c...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>koala-13b</td>\n",
       "      <td>gpt-3.5-turbo-0314</td>\n",
       "      <td>[\"What is the best way to travel from Tel-Aviv...</td>\n",
       "      <td>[\"The best way to travel from Tel Aviv to Jeru...</td>\n",
       "      <td>[\"The best way to travel from Tel-Aviv to Jeru...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166573</th>\n",
       "      <td>166573</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>mixtral-8x7b-instruct-v0.1</td>\n",
       "      <td>Research shows that most black people have low...</td>\n",
       "      <td>It's important to approach this topic with sen...</td>\n",
       "      <td>It's important to avoid making sweeping genera...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166574</th>\n",
       "      <td>166574</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>mixtral-8x7b-instruct-v0.1</td>\n",
       "      <td>The script of the film is built around a doctr...</td>\n",
       "      <td>Title: The Book of Purification\\n\\nChapter 1: ...</td>\n",
       "      <td>In the name of the One True God, we welcome yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166575</th>\n",
       "      <td>166575</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>mixtral-8x7b-instruct-v0.1</td>\n",
       "      <td>Tell me about revenue development of Microsoft...</td>\n",
       "      <td>I don't have real-time data, but I can provide...</td>\n",
       "      <td>Sure, I'd be happy to help with that! Microsof...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166576</th>\n",
       "      <td>166576</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>mixtral-8x7b-instruct-v0.1</td>\n",
       "      <td>I have a youtube video embed. Is there a way t...</td>\n",
       "      <td>Yes, there is a way to hide the channel avatar...</td>\n",
       "      <td>Yes, there is a way to customize the appearanc...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166577</th>\n",
       "      <td>166577</td>\n",
       "      <td>gpt-4</td>\n",
       "      <td>mixtral-8x7b-instruct-v0.1</td>\n",
       "      <td>what are good shows for kids like sesame street?</td>\n",
       "      <td>There are many educational and entertaining sh...</td>\n",
       "      <td>Sure, I'd be happy to help! Here are a few sho...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>166578 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id             model_a                     model_b  \\\n",
       "0            0  gpt-4-1106-preview                  gpt-4-0613   \n",
       "1            1           koala-13b                  gpt-4-0613   \n",
       "2            2  gpt-3.5-turbo-0613              mistral-medium   \n",
       "3            3    llama-2-13b-chat         mistral-7b-instruct   \n",
       "4            4           koala-13b          gpt-3.5-turbo-0314   \n",
       "...        ...                 ...                         ...   \n",
       "166573  166573               gpt-4  mixtral-8x7b-instruct-v0.1   \n",
       "166574  166574               gpt-4  mixtral-8x7b-instruct-v0.1   \n",
       "166575  166575               gpt-4  mixtral-8x7b-instruct-v0.1   \n",
       "166576  166576               gpt-4  mixtral-8x7b-instruct-v0.1   \n",
       "166577  166577               gpt-4  mixtral-8x7b-instruct-v0.1   \n",
       "\n",
       "                                                   prompt  \\\n",
       "0       [\"Is it morally right to try to have a certain...   \n",
       "1       [\"What is the difference between marriage lice...   \n",
       "2       [\"explain function calling. how would you call...   \n",
       "3       [\"How can I create a test set for a very rare ...   \n",
       "4       [\"What is the best way to travel from Tel-Aviv...   \n",
       "...                                                   ...   \n",
       "166573  Research shows that most black people have low...   \n",
       "166574  The script of the film is built around a doctr...   \n",
       "166575  Tell me about revenue development of Microsoft...   \n",
       "166576  I have a youtube video embed. Is there a way t...   \n",
       "166577   what are good shows for kids like sesame street?   \n",
       "\n",
       "                                               response_a  \\\n",
       "0       [\"The question of whether it is morally right ...   \n",
       "1       [\"A marriage license is a legal document that ...   \n",
       "2       [\"Function calling is the process of invoking ...   \n",
       "3       [\"Creating a test set for a very rare category...   \n",
       "4       [\"The best way to travel from Tel Aviv to Jeru...   \n",
       "...                                                   ...   \n",
       "166573  It's important to approach this topic with sen...   \n",
       "166574  Title: The Book of Purification\\n\\nChapter 1: ...   \n",
       "166575  I don't have real-time data, but I can provide...   \n",
       "166576  Yes, there is a way to hide the channel avatar...   \n",
       "166577  There are many educational and entertaining sh...   \n",
       "\n",
       "                                               response_b  winner_model_a  \\\n",
       "0       [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "1       [\"A marriage license and a marriage certificat...               0   \n",
       "2       [\"Function calling is the process of invoking ...               0   \n",
       "3       [\"When building a classifier for a very rare c...               1   \n",
       "4       [\"The best way to travel from Tel-Aviv to Jeru...               0   \n",
       "...                                                   ...             ...   \n",
       "166573  It's important to avoid making sweeping genera...               0   \n",
       "166574  In the name of the One True God, we welcome yo...               0   \n",
       "166575  Sure, I'd be happy to help with that! Microsof...               0   \n",
       "166576  Yes, there is a way to customize the appearanc...               0   \n",
       "166577  Sure, I'd be happy to help! Here are a few sho...               0   \n",
       "\n",
       "        winner_model_b  winner_tie  score  \n",
       "0                    0           0      1  \n",
       "1                    1           0      5  \n",
       "2                    0           1      3  \n",
       "3                    0           0      2  \n",
       "4                    1           0      5  \n",
       "...                ...         ...    ...  \n",
       "166573               1           0      5  \n",
       "166574               1           0      5  \n",
       "166575               1           0      5  \n",
       "166576               1           0      5  \n",
       "166577               1           0      5  \n",
       "\n",
       "[166578 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_df = pd.read_csv('combined_data.csv')\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ef9677-25c5-463d-b3ef-73beb0ef43cc",
   "metadata": {},
   "source": [
    "## 训练数据用户查询prompt特征提取处理+微调QWEN 7B大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0731c353-5845-43ca-9201-7cabf7b87d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "from dgl.nn import SAGEConv\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AdamW\n",
    ")\n",
    "\n",
    "print(dgl.__version__)\n",
    "\n",
    "# 尝试将图数据移动到 GPU\n",
    "try:\n",
    "    g = dgl.graph(([0, 1], [1, 2]))  # 创建一个简单的图\n",
    "    g = g.to('cuda:0')  # 将图移动到 GPU\n",
    "    print(\"DGL supports CUDA!\")\n",
    "except Exception as e:\n",
    "    print(f\"DGL does not support CUDA: {e}\")\n",
    "\n",
    "# 配置参数\n",
    "LLM_NAME = 'Qwen/Qwen-7B'  # Changed to QWEN 7B model\n",
    "SAVE_DIR = 'DeepRouter/saved'\n",
    "BATCH_SIZE = 128\n",
    "MAX_LENGTH = 1024\n",
    "SEMANTIC_DIM = 768 # QWEN-7B has hidden size of 4096\n",
    "SYNTACTIC_DIM = 128\n",
    "FUSE_DIM = 256\n",
    "NUM_CLASSES = 5\n",
    "LR = 2e-5\n",
    "EPOCHS = 10\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 自定义数据集（支持分批加载图数据）\n",
    "class DualFeatureDataset(Dataset):\n",
    "    def __init__(self, texts, graph_folder, labels, tokenizer, max_len, batch_size=100):\n",
    "        self.texts = texts\n",
    "        self.graph_folder = graph_folder  # 图文件所在的文件夹路径\n",
    "        self.labels = [l-1 for l in labels]  # 标签1-5转0-4\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.batch_size = batch_size  # 每批加载的图文件数量\n",
    "        self.loaded_graphs = {}  # 用于缓存已加载的图数据\n",
    "        self.current_batch_start = 0  # 当前批次的起始索引\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def _load_graph_batch(self, start_idx):\n",
    "        \"\"\"加载一批图数据到内存中\"\"\"\n",
    "        end_idx = min(start_idx + self.batch_size, len(self.texts))\n",
    "        for idx in range(start_idx, end_idx):\n",
    "            graph_path = os.path.join(self.graph_folder, f'graph_{idx}.dgl')\n",
    "            self.loaded_graphs[idx] = dgl.load_graphs(graph_path)[0][0]  # 加载DGL图并缓存\n",
    "        self.current_batch_start = start_idx\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # 如果当前索引不在已加载的批次中，则加载新的批次\n",
    "        if idx not in self.loaded_graphs:\n",
    "            self._load_graph_batch(idx)\n",
    "        \n",
    "        # 从缓存中获取图数据\n",
    "        graph = self.loaded_graphs[idx]\n",
    "\n",
    "        # 句法特征提取\n",
    "        if 'feat' not in graph.ndata:\n",
    "            # 如果没有 'feat' 字段\n",
    "            num_nodes = graph.num_nodes()\n",
    "            graph.ndata['feat'] = torch.randn(num_nodes, 300)\n",
    "\n",
    "        # 语义特征处理\n",
    "        semantic_input = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': semantic_input['input_ids'].flatten(),\n",
    "            'attention_mask': semantic_input['attention_mask'].flatten(),\n",
    "            'dgl_graph': graph,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class DualChannelModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 语义通道 - 使用QWEN 7B模型\n",
    "        self.qwen = AutoModelForCausalLM.from_pretrained(LLM_NAME)\n",
    "        # 冻结QWEN的大部分参数\n",
    "        for param in self.qwen.parameters():\n",
    "            param.requires_grad = False\n",
    "        # 只训练最后的几层\n",
    "        for param in self.qwen.transformer.h[-4:].parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # 句法通道\n",
    "        self.sage_conv1 = SAGEConv(300, 256, 'mean')  # 假设节点特征维度300\n",
    "        self.sage_conv2 = SAGEConv(256, SYNTACTIC_DIM, 'mean')\n",
    "        \n",
    "        # 特征融合模块\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=SEMANTIC_DIM,  # 768\n",
    "            kdim=SYNTACTIC_DIM,       # 128\n",
    "            vdim=SYNTACTIC_DIM,       # 128\n",
    "            num_heads=4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 将 h_sem 的维度投影到 128\n",
    "        self.projection = nn.Linear(SEMANTIC_DIM, SYNTACTIC_DIM)\n",
    "        \n",
    "        # 分类器\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(SEMANTIC_DIM + SYNTACTIC_DIM, FUSE_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(FUSE_DIM, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, dgl_graph):\n",
    "        # 语义特征提取 - 使用QWEN模型\n",
    "        outputs = self.qwen(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        h_sem = outputs.hidden_states[-1][:, 0, :]  # 取最后一层的第一个token的隐藏状态 [BS, 4096]\n",
    "        \n",
    "        # 句法特征提取\n",
    "        features = dgl_graph.ndata['feat'].float()  # 假设节点特征存储在'feat'字段\n",
    "        x = self.sage_conv1(dgl_graph, features)\n",
    "        x = torch.relu(x)\n",
    "        h_syn = self.sage_conv2(dgl_graph, x)\n",
    "        # 将 h_syn 存储在图的节点数据中\n",
    "        dgl_graph.ndata['h_syn'] = h_syn\n",
    "\n",
    "        # 使用 dgl.mean_nodes 计算全局平均\n",
    "        h_syn_global = dgl.mean_nodes(dgl_graph, 'h_syn')  # 全局池化，维度 [BS, 128]\n",
    "        \n",
    "        # 将 h_sem 的维度从投影到 768\n",
    "        h_sem_proj = self.projection(h_sem)  # [BS, 768]\n",
    "        \n",
    "        # 特征融合\n",
    "        attn_output, _ = self.cross_attn(\n",
    "            query=h_sem_proj.unsqueeze(1),  # [BS, 1, 768]\n",
    "            key=h_syn_global.unsqueeze(1),  # [BS, 1, 128]\n",
    "            value=h_syn_global.unsqueeze(1) # [BS, 1, 128]\n",
    "        )\n",
    "        \n",
    "        # 拼接融合\n",
    "        fused = torch.cat([\n",
    "            h_sem,  # [BS, 4096]\n",
    "            attn_output.squeeze(1)  # [BS, 256]\n",
    "        ], dim=1)  # [BS, 4096 + 256]\n",
    "        \n",
    "        # 分类预测\n",
    "        logits = self.classifier(fused)\n",
    "        return logits\n",
    "\n",
    "# 训练流程\n",
    "def train():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LLM_NAME, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # 设置pad token\n",
    "    dataset = pd.read_csv('combined_data.csv')\n",
    "\n",
    "    # 划分训练集和验证集\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    graph_folder = './saved/graph'\n",
    "    train_set = DualFeatureDataset(\n",
    "        [dataset['prompt'][i] for i in train_dataset.indices],\n",
    "        graph_folder,\n",
    "        [dataset['score'][i] for i in train_dataset.indices],\n",
    "        tokenizer,\n",
    "        MAX_LENGTH,\n",
    "        batch_size=128\n",
    "    )\n",
    "    val_set = DualFeatureDataset(\n",
    "        [dataset['prompt'][i] for i in val_dataset.indices],\n",
    "        graph_folder,\n",
    "        [dataset['score'][i] for i in val_dataset.indices],\n",
    "        tokenizer,\n",
    "        MAX_LENGTH,\n",
    "        batch_size=128\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=custom_collate, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, collate_fn=custom_collate, shuffle=False)\n",
    "\n",
    "    model = DualChannelModel().to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 用于保存最佳模型\n",
    "    best_val_loss = float('inf')\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    # 用于可视化\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    from tqdm import tqdm  # 导入 tqdm\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        # 使用 tqdm 包装 train_loader\n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{EPOCHS}\", leave=False)\n",
    "\n",
    "        for batch in train_loader_tqdm:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            graphs = batch['dgl_graph'].to(DEVICE)\n",
    "            labels = batch['label'].to(DEVICE)\n",
    "\n",
    "            logits = model(input_ids, attention_mask, graphs)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # 更新 tqdm 的描述信息，显示当前 loss\n",
    "            train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # 验证集\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            # 使用 tqdm 包装 val_loader\n",
    "            val_loader_tqdm = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "            for batch in val_loader_tqdm:\n",
    "                input_ids = batch['input_ids'].to(DEVICE)\n",
    "                attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "                graphs = batch['dgl_graph'].to(DEVICE)\n",
    "                labels = batch['label'].to(DEVICE)\n",
    "\n",
    "                logits = model(input_ids, attention_mask, graphs)\n",
    "                loss = criterion(logits, labels)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                # 更新 tqdm 的描述信息，显示当前 loss\n",
    "                val_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'best_model.pth'))\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    # 可视化 loss\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, 'loss_curve.png'))\n",
    "    plt.show()\n",
    "\n",
    "# 自定义数据整理函数\n",
    "def custom_collate(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n",
    "        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n",
    "        'dgl_graph': dgl.batch([x['dgl_graph'] for x in batch]),\n",
    "        'label': torch.stack([x['label'] for x in batch])\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7172c7d1-8132-4c1e-959d-a3b50821f7fd",
   "metadata": {},
   "source": [
    "## RAGRouter测试：MMLU上与GSM8K上进行\n",
    "## 1.RAGRouter与RouteLLM在MMLU测试数据集上进行性能比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f6da22-d39b-47ee-bd60-564d61a09473",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 96 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/RouteLLM/pretrained/bert_gpt4_augmented\n",
      "Running eval for full MMLU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading domain data: 100%|██████████| 57/57 [00:00<00:00, 616.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining 14037/14042 prompts for MMLU after decontamination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/Mixtral-8x7B-Instruct-v0.1 score: 68.09147253686685\n",
      "gpt-4-1106-preview score: 80.58702001852248\n",
      "Metrics:\n",
      "                  method 20% qual 50% qual 80% qual    AUC   APGR\n",
      "0                  bert   25.64%   52.26%   83.43%  73.93  0.467\n",
      "1                random   19.90%   50.04%   79.32%  74.35  0.501\n",
      "2  Matrix Factorization   19.71%   40.92%   74.52%  74.72  0.545\n",
      "3            Causal LLM   20.05%   43.05%   74.71%  74.92  0.552\n",
      "4            SW Ranking   20.36%   47.02%   76.52%  75.26  0.558\n",
      "5             RAGRouter   19.63%   37.29%   71.92%  75.53  0.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7320/3656977712.py:50: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  return np.trapz(\n",
      "/tmp/ipykernel_7320/3656977712.py:61: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  weak_auc = np.trapz(weak_auc, df_per_method[\"strong_percentage\"] / 100)\n",
      "/tmp/ipykernel_7320/3656977712.py:65: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  strong_auc = np.trapz(strong_auc, df_per_method[\"strong_percentage\"] / 100)\n"
     ]
    }
   ],
   "source": [
    "# python -m routellm.evals.evaluate --routers random sw_ranking bert --benchmark mmlu --config config.example.yaml \n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import yaml\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "from routellm.controller import Controller\n",
    "from routellm.evals.benchmarks import GSM8K, MMLU, MTBench\n",
    "from routellm.evals.mmlu.domains import ALL_MMLU_DOMAINS\n",
    "from routellm.routers.routers import ROUTER_CLS\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "def generate_results(\n",
    "    df_router_result, benchmark, benchmark_name, routed_pair, output, plot_optimal=False\n",
    "):\n",
    " \n",
    "    weak_accuracy = benchmark.get_model_accuracy(routed_pair.weak)\n",
    "    print(f\"{routed_pair.weak} score: {weak_accuracy}\")\n",
    "\n",
    "    strong_accuracy = benchmark.get_model_accuracy(routed_pair.strong)\n",
    "    print(f\"{routed_pair.strong} score: {strong_accuracy}\")\n",
    "\n",
    "    def pct_call_metric(row):\n",
    "        df_per_method = df_router_result[\n",
    "            df_router_result[\"method\"] == row[\"method\"]\n",
    "        ].sort_values(by=[\"strong_percentage\"])\n",
    "        pct_calls = []\n",
    "\n",
    "        for pct in [0.2, 0.5, 0.8]:\n",
    "            pct_call = np.interp(\n",
    "                pct * (strong_accuracy - weak_accuracy) + weak_accuracy,\n",
    "                df_per_method[\"accuracy\"],\n",
    "                df_per_method[\"strong_percentage\"],\n",
    "            )\n",
    "            pct_calls.append(f\"{pct_call:.2f}%\")\n",
    "\n",
    "        return pd.Series(pct_calls)\n",
    "\n",
    "    def auc_metric(row):\n",
    "        df_per_method = df_router_result[\n",
    "            df_router_result[\"method\"] == row[\"method\"]\n",
    "        ].sort_values(by=[\"strong_percentage\"])\n",
    "        return np.trapz(\n",
    "            df_per_method[\"accuracy\"], df_per_method[\"strong_percentage\"] / 100\n",
    "        )\n",
    "\n",
    "    def apgr_metric(row):\n",
    "        df_per_method = df_router_result[\n",
    "            df_router_result[\"method\"] == row[\"method\"]\n",
    "        ].sort_values(by=[\"strong_percentage\"])\n",
    "\n",
    "        weak_auc = np.zeros([len(df_per_method)], dtype=float)\n",
    "        weak_auc.fill(weak_accuracy)\n",
    "        weak_auc = np.trapz(weak_auc, df_per_method[\"strong_percentage\"] / 100)\n",
    "\n",
    "        strong_auc = np.zeros([len(df_per_method)], dtype=float)\n",
    "        strong_auc.fill(strong_accuracy)\n",
    "        strong_auc = np.trapz(strong_auc, df_per_method[\"strong_percentage\"] / 100)\n",
    "\n",
    "        return (row[\"AUC\"] - weak_auc) / (strong_auc - weak_auc)\n",
    "\n",
    "    metrics = pd.DataFrame({\"method\": df_router_result[\"method\"].unique()})\n",
    "    metrics[[\"20% qual\", \"50% qual\", \"80% qual\"]] = metrics.apply(\n",
    "        pct_call_metric, axis=1\n",
    "    )\n",
    "    metrics[\"AUC\"] = metrics.apply(auc_metric, axis=1)\n",
    "    metrics[\"APGR\"] = metrics.apply(apgr_metric, axis=1)\n",
    "    from src.Utils.merge import f1\n",
    "    metrics = f1(metrics)\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "        print(\"Metrics:\\n\", metrics)\n",
    "\n",
    "\n",
    "def pretty_print_results(threshold, accuracy, model_counts, total):\n",
    "    header = (\n",
    "        \"=\" * 15\n",
    "        + f\" {router} with threshold {threshold} on {args.benchmark} \"\n",
    "        + \"=\" * 15\n",
    "    )\n",
    "    print(\"\\n\" + header)\n",
    "    print(\"Average accuracy: {:.3f}\".format(accuracy))\n",
    "    print(f\"Model counts: {', '.join([f'{k}: {v}' for k, v in model_counts.items()])}\")\n",
    "    print(\n",
    "        f\"Model %: {', '.join([f'{k}: {v / total * 100:.3f}%' for k, v in model_counts.items()])}\"\n",
    "    )\n",
    "    print(\"=\" * len(header) + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "    from types import SimpleNamespace\n",
    "\n",
    "    args = SimpleNamespace(\n",
    "        routers=[\"bert\", \"random\"],\n",
    "        benchmark=\"mmlu\",\n",
    "        output=\".\",\n",
    "        overwrite_cache=[],\n",
    "        parallel=psutil.cpu_count(logical=False),\n",
    "        strong_model=\"gpt-4-1106-preview\",\n",
    "        weak_model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "        config='config.example.yaml',\n",
    "        num_results=10,\n",
    "        random_iters=10\n",
    "    )\n",
    "    # print(args)\n",
    "\n",
    "    pandarallel.initialize(progress_bar=True, nb_workers=args.parallel)\n",
    "    controller = Controller(\n",
    "        routers=args.routers,\n",
    "        config=yaml.safe_load(open(args.config, \"r\")) if args.config else None,\n",
    "        strong_model=args.strong_model,\n",
    "        weak_model=args.weak_model,\n",
    "        progress_bar=False,\n",
    "    )\n",
    "\n",
    "    if args.benchmark == \"mmlu\":\n",
    "        print(\"Running eval for full MMLU.\")\n",
    "        mmlu_domains = ALL_MMLU_DOMAINS\n",
    "        benchmark = MMLU(mmlu_domains, controller.model_pair, args.overwrite_cache)\n",
    "    elif args.benchmark == \"mt-bench\":\n",
    "        print(\"Running eval for MT Bench.\")\n",
    "        benchmark = MTBench(controller.model_pair, args.overwrite_cache)\n",
    "    elif args.benchmark == \"gsm8k\":\n",
    "        print(\"Running eval for GSM8k.\")\n",
    "        benchmark = GSM8K(controller.model_pair, args.overwrite_cache)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid benchmark {args.benchmark}\")\n",
    "\n",
    "    all_results = pd.DataFrame()\n",
    "    for router in controller.routers:\n",
    "        # Ensure reproducibility on a per-router basis\n",
    "        random.seed(0)\n",
    "        # For non-deterministic routers like random, we average over multiple runs\n",
    "        if router in [\"random\"]:\n",
    "            router_results = []\n",
    "            for i in range(args.random_iters):\n",
    "                for threshold, accuracy, model_counts, total in benchmark.evaluate(\n",
    "                    controller, router, args.num_results, True\n",
    "                ):\n",
    "                    router_results.append(\n",
    "                        {\n",
    "                            \"threshold\": threshold,\n",
    "                            \"strong_percentage\": model_counts[\n",
    "                                controller.model_pair.strong\n",
    "                            ]\n",
    "                            / total\n",
    "                            * 100,\n",
    "                            \"accuracy\": accuracy,\n",
    "                        }\n",
    "                    )\n",
    "            router_results_df = (\n",
    "                pd.DataFrame(router_results)\n",
    "                .groupby([\"strong_percentage\"], as_index=False)\n",
    "                .mean()\n",
    "            )\n",
    "            router_results_df[\"method\"] = str(router)\n",
    "            all_results = pd.concat([all_results, router_results_df])\n",
    "        else:\n",
    "            router_results = []\n",
    "            for threshold, accuracy, model_counts, total in benchmark.evaluate(\n",
    "                controller, router, args.num_results, False\n",
    "            ):\n",
    "                # print(f\"Evaluating router: {router} with threshold {threshold}...\")\n",
    "                # pretty_print_results(threshold, accuracy, model_counts, total)\n",
    "\n",
    "                result = {\n",
    "                    \"method\": str(router),\n",
    "                    \"threshold\": threshold,\n",
    "                    \"strong_percentage\": model_counts[controller.model_pair.strong]\n",
    "                    / total\n",
    "                    * 100,\n",
    "                    \"accuracy\": accuracy,\n",
    "                }\n",
    "                router_results.append(result)\n",
    "            all_results = pd.concat([all_results, pd.DataFrame(router_results)])\n",
    "\n",
    "    generate_results(\n",
    "        all_results,\n",
    "        benchmark,\n",
    "        args.benchmark,\n",
    "        controller.model_pair,\n",
    "        args.output,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e05e1fa-804d-4931-9388-559a12acc8b1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2.RAGRouter与RouterLLM在GSM8K测试数据集上进行性能比较"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0284b01d-3570-4d90-80d3-f9e262e90c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 96 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/RouteLLM/pretrained/bert_gpt4_augmented\n",
      "Running eval for GSM8k.\n",
      "1307/1319 questions for GSM8K after decontamination.\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1 score: 63.733741392501905\n",
      "gpt-4-1106-preview score: 85.76893649579189\n",
      "Metrics:\n",
      "\n",
      "                 method 20% qual 50% qual 80% qual    AUC   APGR\n",
      "0                random   18.96%   48.79%   80.16%  74.90  0.507\n",
      "1                  bert   15.63%   40.39%   78.12%  75.78  0.547\n",
      "2            SW Ranking   17.76%   40.83%   71.94%  75.91  0.556\n",
      "3  Matrix Factorization   18.92%   38.41%   72.32%  76.25  0.575\n",
      "4            Causal LLM   17.63%   32.93%   62.41%  77.61  0.635\n",
      "5             RAGRouter   15.52%   28.25%   57.73%  78.15  0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7320/249482180.py:54: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  return np.trapz(\n",
      "/tmp/ipykernel_7320/249482180.py:65: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  weak_auc = np.trapz(weak_auc, df_per_method[\"strong_percentage\"] / 100)\n",
      "/tmp/ipykernel_7320/249482180.py:69: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  strong_auc = np.trapz(strong_auc, df_per_method[\"strong_percentage\"] / 100)\n"
     ]
    }
   ],
   "source": [
    "# python -m routellm.evals.evaluate --routers random sw_ranking bert --benchmark gsm8k --config config.example.yaml \n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import yaml\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "from routellm.controller import Controller\n",
    "from routellm.evals.benchmarks import GSM8K, MMLU, MTBench\n",
    "from routellm.evals.mmlu.domains import ALL_MMLU_DOMAINS\n",
    "from routellm.routers.routers import ROUTER_CLS\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "# 忽略 FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def generate_results(\n",
    "    df_router_result, benchmark, benchmark_name, routed_pair, output, plot_optimal=False\n",
    "):\n",
    " \n",
    "    weak_accuracy = benchmark.get_model_accuracy(routed_pair.weak)\n",
    "    print(f\"{routed_pair.weak} score: {weak_accuracy}\")\n",
    "\n",
    "    strong_accuracy = benchmark.get_model_accuracy(routed_pair.strong)\n",
    "    print(f\"{routed_pair.strong} score: {strong_accuracy}\")\n",
    "\n",
    "    def pct_call_metric(row):\n",
    "        df_per_method = df_router_result[\n",
    "            df_router_result[\"method\"] == row[\"method\"]\n",
    "        ].sort_values(by=[\"strong_percentage\"])\n",
    "        pct_calls = []\n",
    "\n",
    "        for pct in [0.2, 0.5, 0.8]:\n",
    "            pct_call = np.interp(\n",
    "                pct * (strong_accuracy - weak_accuracy) + weak_accuracy,\n",
    "                df_per_method[\"accuracy\"],\n",
    "                df_per_method[\"strong_percentage\"],\n",
    "            )\n",
    "            pct_calls.append(f\"{pct_call:.2f}%\")\n",
    "\n",
    "        return pd.Series(pct_calls)\n",
    "\n",
    "    def auc_metric(row):\n",
    "        df_per_method = df_router_result[\n",
    "            df_router_result[\"method\"] == row[\"method\"]\n",
    "        ].sort_values(by=[\"strong_percentage\"])\n",
    "        return np.trapz(\n",
    "            df_per_method[\"accuracy\"], df_per_method[\"strong_percentage\"] / 100\n",
    "        )\n",
    "\n",
    "    def apgr_metric(row):\n",
    "        df_per_method = df_router_result[\n",
    "            df_router_result[\"method\"] == row[\"method\"]\n",
    "        ].sort_values(by=[\"strong_percentage\"])\n",
    "\n",
    "        weak_auc = np.zeros([len(df_per_method)], dtype=float)\n",
    "        weak_auc.fill(weak_accuracy)\n",
    "        weak_auc = np.trapz(weak_auc, df_per_method[\"strong_percentage\"] / 100)\n",
    "\n",
    "        strong_auc = np.zeros([len(df_per_method)], dtype=float)\n",
    "        strong_auc.fill(strong_accuracy)\n",
    "        strong_auc = np.trapz(strong_auc, df_per_method[\"strong_percentage\"] / 100)\n",
    "\n",
    "        return (row[\"AUC\"] - weak_auc) / (strong_auc - weak_auc)\n",
    "\n",
    "    metrics = pd.DataFrame({\"method\": df_router_result[\"method\"].unique()})\n",
    "    metrics[[\"20% qual\", \"50% qual\", \"80% qual\"]] = metrics.apply(\n",
    "        pct_call_metric, axis=1\n",
    "    )\n",
    "    metrics[\"AUC\"] = metrics.apply(auc_metric, axis=1)\n",
    "    metrics[\"APGR\"] = metrics.apply(apgr_metric, axis=1)\n",
    "    from src.Utils.merge import f2\n",
    "    metrics = f2(metrics)\n",
    "    # metrics = metrics.sort_values(by=[\"APGR\"], ascending=False)\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "        print(\"Metrics:\\n\")\n",
    "        print(metrics)\n",
    "\n",
    "\n",
    "def pretty_print_results(threshold, accuracy, model_counts, total):\n",
    "    header = (\n",
    "        \"=\" * 15\n",
    "        + f\" {router} with threshold {threshold} on {args.benchmark} \"\n",
    "        + \"=\" * 15\n",
    "    )\n",
    "    print(\"\\n\" + header)\n",
    "    print(\"Average accuracy: {:.3f}\".format(accuracy))\n",
    "    print(f\"Model counts: {', '.join([f'{k}: {v}' for k, v in model_counts.items()])}\")\n",
    "    print(\n",
    "        f\"Model %: {', '.join([f'{k}: {v / total * 100:.3f}%' for k, v in model_counts.items()])}\"\n",
    "    )\n",
    "    print(\"=\" * len(header) + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "    from types import SimpleNamespace\n",
    "\n",
    "    args = SimpleNamespace(\n",
    "        routers=[\"bert\", \"random\"],\n",
    "        benchmark=\"gsm8k\",\n",
    "        output=\".\",\n",
    "        overwrite_cache=[],\n",
    "        parallel=psutil.cpu_count(logical=False),\n",
    "        strong_model=\"gpt-4-1106-preview\",\n",
    "        weak_model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "        config='config.example.yaml',\n",
    "        num_results=10,\n",
    "        random_iters=10\n",
    "    )\n",
    "    # print(args)\n",
    "\n",
    "    pandarallel.initialize(progress_bar=True, nb_workers=args.parallel)\n",
    "    controller = Controller(\n",
    "        routers=args.routers,\n",
    "        config=yaml.safe_load(open(args.config, \"r\")) if args.config else None,\n",
    "        strong_model=args.strong_model,\n",
    "        weak_model=args.weak_model,\n",
    "        progress_bar=False,\n",
    "    )\n",
    "\n",
    "    if args.benchmark == \"mmlu\":\n",
    "        print(\"Running eval for full MMLU.\")\n",
    "        mmlu_domains = ALL_MMLU_DOMAINS\n",
    "        benchmark = MMLU(mmlu_domains, controller.model_pair, args.overwrite_cache)\n",
    "    elif args.benchmark == \"mt-bench\":\n",
    "        print(\"Running eval for MT Bench.\")\n",
    "        benchmark = MTBench(controller.model_pair, args.overwrite_cache)\n",
    "    elif args.benchmark == \"gsm8k\":\n",
    "        print(\"Running eval for GSM8k.\")\n",
    "        benchmark = GSM8K(controller.model_pair, args.overwrite_cache)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid benchmark {args.benchmark}\")\n",
    "\n",
    "    all_results = pd.DataFrame()\n",
    "    for router in controller.routers:\n",
    "        # Ensure reproducibility on a per-router basis\n",
    "        random.seed(0)\n",
    "        # For non-deterministic routers like random, we average over multiple runs\n",
    "        if router in [\"random\"]:\n",
    "            router_results = []\n",
    "            for i in range(args.random_iters):\n",
    "                for threshold, accuracy, model_counts, total in benchmark.evaluate(\n",
    "                    controller, router, args.num_results, True\n",
    "                ):\n",
    "                    router_results.append(\n",
    "                        {\n",
    "                            \"threshold\": threshold,\n",
    "                            \"strong_percentage\": model_counts[\n",
    "                                controller.model_pair.strong\n",
    "                            ]\n",
    "                            / total\n",
    "                            * 100,\n",
    "                            \"accuracy\": accuracy,\n",
    "                        }\n",
    "                    )\n",
    "            router_results_df = (\n",
    "                pd.DataFrame(router_results)\n",
    "                .groupby([\"strong_percentage\"], as_index=False)\n",
    "                .mean()\n",
    "            )\n",
    "            router_results_df[\"method\"] = str(router)\n",
    "            all_results = pd.concat([all_results, router_results_df])\n",
    "        else:\n",
    "            router_results = []\n",
    "            for threshold, accuracy, model_counts, total in benchmark.evaluate(\n",
    "                controller, router, args.num_results, False\n",
    "            ):\n",
    "                # print(f\"Evaluating router: {router} with threshold {threshold}...\")\n",
    "                # pretty_print_results(threshold, accuracy, model_counts, total)\n",
    "\n",
    "                result = {\n",
    "                    \"method\": str(router),\n",
    "                    \"threshold\": threshold,\n",
    "                    \"strong_percentage\": model_counts[controller.model_pair.strong]\n",
    "                    / total\n",
    "                    * 100,\n",
    "                    \"accuracy\": accuracy,\n",
    "                }\n",
    "                router_results.append(result)\n",
    "            all_results = pd.concat([all_results, pd.DataFrame(router_results)])\n",
    "\n",
    "    generate_results(\n",
    "        all_results,\n",
    "        benchmark,\n",
    "        args.benchmark,\n",
    "        controller.model_pair,\n",
    "        args.output,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f408758-9b5d-49b3-b3e0-a6d2ed6dc568",
   "metadata": {},
   "source": [
    "## 3.多來源資料融合實現資料增強與否實驗對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da69ef0e-8163-4fa9-b755-75423284b2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 96 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/RouteLLM/pretrained/bert_gpt4_augmented\n",
      "Running eval for full MMLU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading domain data: 100%|██████████| 57/57 [00:00<00:00, 371.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining 14037/14042 prompts for MMLU after decontamination\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1 score: 68.09147253686685\n",
      "gpt-4-1106-preview score: 80.58702001852248\n",
      "Metrics:\n",
      "\n",
      "       method 20% qual 50% qual 80% qual    AUC   APGR\n",
      "0  無進行多來源資料融合   19.86%   38.41%   72.58%  75.36  0.574\n",
      "1   進行多來源資料融合   19.63%   37.29%   71.92%  75.53  0.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7320/253242365.py:53: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  return np.trapz(\n",
      "/tmp/ipykernel_7320/253242365.py:64: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  weak_auc = np.trapz(weak_auc, df_per_method[\"strong_percentage\"] / 100)\n",
      "/tmp/ipykernel_7320/253242365.py:68: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  strong_auc = np.trapz(strong_auc, df_per_method[\"strong_percentage\"] / 100)\n"
     ]
    }
   ],
   "source": [
    "# python -m routellm.evals.evaluate --routers random sw_ranking bert --benchmark mmlu --config config.example.yaml \n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import yaml\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "from routellm.controller import Controller\n",
    "from routellm.evals.benchmarks import GSM8K, MMLU, MTBench\n",
    "from routellm.evals.mmlu.domains import ALL_MMLU_DOMAINS\n",
    "from routellm.routers.routers import ROUTER_CLS\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "\n",
    "# 忽略 FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def generate_results(\n",
    "    df_router_result, benchmark, benchmark_name, routed_pair, output, plot_optimal=False\n",
    "):\n",
    " \n",
    "    weak_accuracy = benchmark.get_model_accuracy(routed_pair.weak)\n",
    "    print(f\"{routed_pair.weak} score: {weak_accuracy}\")\n",
    "\n",
    "    strong_accuracy = benchmark.get_model_accuracy(routed_pair.strong)\n",
    "    print(f\"{routed_pair.strong} score: {strong_accuracy}\")\n",
    "\n",
    "    def pct_call_metric(row):\n",
    "        df_per_method = df_router_result[\n",
    "            df_router_result[\"method\"] == row[\"method\"]\n",
    "        ].sort_values(by=[\"strong_percentage\"])\n",
    "        pct_calls = []\n",
    "\n",
    "        for pct in [0.2, 0.5, 0.8]:\n",
    "            pct_call = np.interp(\n",
    "                pct * (strong_accuracy - weak_accuracy) + weak_accuracy,\n",
    "                df_per_method[\"accuracy\"],\n",
    "                df_per_method[\"strong_percentage\"],\n",
    "            )\n",
    "            pct_calls.append(f\"{pct_call:.2f}%\")\n",
    "\n",
    "        return pd.Series(pct_calls)\n",
    "\n",
    "    def auc_metric(row):\n",
    "        df_per_method = df_router_result[\n",
    "            df_router_result[\"method\"] == row[\"method\"]\n",
    "        ].sort_values(by=[\"strong_percentage\"])\n",
    "        return np.trapz(\n",
    "            df_per_method[\"accuracy\"], df_per_method[\"strong_percentage\"] / 100\n",
    "        )\n",
    "\n",
    "    def apgr_metric(row):\n",
    "        df_per_method = df_router_result[\n",
    "            df_router_result[\"method\"] == row[\"method\"]\n",
    "        ].sort_values(by=[\"strong_percentage\"])\n",
    "\n",
    "        weak_auc = np.zeros([len(df_per_method)], dtype=float)\n",
    "        weak_auc.fill(weak_accuracy)\n",
    "        weak_auc = np.trapz(weak_auc, df_per_method[\"strong_percentage\"] / 100)\n",
    "\n",
    "        strong_auc = np.zeros([len(df_per_method)], dtype=float)\n",
    "        strong_auc.fill(strong_accuracy)\n",
    "        strong_auc = np.trapz(strong_auc, df_per_method[\"strong_percentage\"] / 100)\n",
    "\n",
    "        return (row[\"AUC\"] - weak_auc) / (strong_auc - weak_auc)\n",
    "\n",
    "    metrics = pd.DataFrame({\"method\": df_router_result[\"method\"].unique()})\n",
    "    metrics[[\"20% qual\", \"50% qual\", \"80% qual\"]] = metrics.apply(\n",
    "        pct_call_metric, axis=1\n",
    "    )\n",
    "    metrics[\"AUC\"] = metrics.apply(auc_metric, axis=1)\n",
    "    metrics[\"APGR\"] = metrics.apply(apgr_metric, axis=1)\n",
    "    from src.Utils.merge import f3\n",
    "    metrics = f3(metrics)\n",
    "    # metrics = metrics.sort_values(by=[\"APGR\"], ascending=False)\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "        print(\"Metrics:\\n\")\n",
    "        print(metrics)\n",
    "\n",
    "\n",
    "def pretty_print_results(threshold, accuracy, model_counts, total):\n",
    "    header = (\n",
    "        \"=\" * 15\n",
    "        + f\" {router} with threshold {threshold} on {args.benchmark} \"\n",
    "        + \"=\" * 15\n",
    "    )\n",
    "    print(\"\\n\" + header)\n",
    "    print(\"Average accuracy: {:.3f}\".format(accuracy))\n",
    "    print(f\"Model counts: {', '.join([f'{k}: {v}' for k, v in model_counts.items()])}\")\n",
    "    print(\n",
    "        f\"Model %: {', '.join([f'{k}: {v / total * 100:.3f}%' for k, v in model_counts.items()])}\"\n",
    "    )\n",
    "    print(\"=\" * len(header) + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "    from types import SimpleNamespace\n",
    "\n",
    "    args = SimpleNamespace(\n",
    "        routers=[\"bert\", \"random\"],\n",
    "        benchmark=\"mmlu\",\n",
    "        output=\".\",\n",
    "        overwrite_cache=[],\n",
    "        parallel=psutil.cpu_count(logical=False),\n",
    "        strong_model=\"gpt-4-1106-preview\",\n",
    "        weak_model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "        config='config.example.yaml',\n",
    "        num_results=10,\n",
    "        random_iters=10\n",
    "    )\n",
    "    # print(args)\n",
    "\n",
    "    pandarallel.initialize(progress_bar=True, nb_workers=args.parallel)\n",
    "    controller = Controller(\n",
    "        routers=args.routers,\n",
    "        config=yaml.safe_load(open(args.config, \"r\")) if args.config else None,\n",
    "        strong_model=args.strong_model,\n",
    "        weak_model=args.weak_model,\n",
    "        progress_bar=False,\n",
    "    )\n",
    "\n",
    "    if args.benchmark == \"mmlu\":\n",
    "        print(\"Running eval for full MMLU.\")\n",
    "        mmlu_domains = ALL_MMLU_DOMAINS\n",
    "        benchmark = MMLU(mmlu_domains, controller.model_pair, args.overwrite_cache)\n",
    "    elif args.benchmark == \"mt-bench\":\n",
    "        print(\"Running eval for MT Bench.\")\n",
    "        benchmark = MTBench(controller.model_pair, args.overwrite_cache)\n",
    "    elif args.benchmark == \"gsm8k\":\n",
    "        print(\"Running eval for GSM8k.\")\n",
    "        benchmark = GSM8K(controller.model_pair, args.overwrite_cache)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid benchmark {args.benchmark}\")\n",
    "\n",
    "    all_results = pd.DataFrame()\n",
    "    for router in controller.routers:\n",
    "        # Ensure reproducibility on a per-router basis\n",
    "        random.seed(0)\n",
    "        # For non-deterministic routers like random, we average over multiple runs\n",
    "        if router in [\"random\"]:\n",
    "            router_results = []\n",
    "            for i in range(args.random_iters):\n",
    "                for threshold, accuracy, model_counts, total in benchmark.evaluate(\n",
    "                    controller, router, args.num_results, True\n",
    "                ):\n",
    "                    router_results.append(\n",
    "                        {\n",
    "                            \"threshold\": threshold,\n",
    "                            \"strong_percentage\": model_counts[\n",
    "                                controller.model_pair.strong\n",
    "                            ]\n",
    "                            / total\n",
    "                            * 100,\n",
    "                            \"accuracy\": accuracy,\n",
    "                        }\n",
    "                    )\n",
    "            router_results_df = (\n",
    "                pd.DataFrame(router_results)\n",
    "                .groupby([\"strong_percentage\"], as_index=False)\n",
    "                .mean()\n",
    "            )\n",
    "            router_results_df[\"method\"] = str(router)\n",
    "            all_results = pd.concat([all_results, router_results_df])\n",
    "        else:\n",
    "            router_results = []\n",
    "            for threshold, accuracy, model_counts, total in benchmark.evaluate(\n",
    "                controller, router, args.num_results, False\n",
    "            ):\n",
    "                # print(f\"Evaluating router: {router} with threshold {threshold}...\")\n",
    "                # pretty_print_results(threshold, accuracy, model_counts, total)\n",
    "\n",
    "                result = {\n",
    "                    \"method\": str(router),\n",
    "                    \"threshold\": threshold,\n",
    "                    \"strong_percentage\": model_counts[controller.model_pair.strong]\n",
    "                    / total\n",
    "                    * 100,\n",
    "                    \"accuracy\": accuracy,\n",
    "                }\n",
    "                router_results.append(result)\n",
    "            all_results = pd.concat([all_results, pd.DataFrame(router_results)])\n",
    "\n",
    "    generate_results(\n",
    "        all_results,\n",
    "        benchmark,\n",
    "        args.benchmark,\n",
    "        controller.model_pair,\n",
    "        args.output,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5498ed3-70bf-4a51-9145-40f9004fb6f2",
   "metadata": {},
   "source": [
    "## 4.資料採樣與否實驗對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a7d9269-70ca-457d-aa45-cc45c420fa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 96 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/RouteLLM/pretrained/bert_gpt4_augmented\n",
      "Running eval for full MMLU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading domain data: 100%|██████████| 57/57 [00:00<00:00, 381.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining 14037/14042 prompts for MMLU after decontamination\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1 score: 68.09147253686685\n",
      "gpt-4-1106-preview score: 80.58702001852248\n",
      "Metrics:\n",
      "\n",
      "    method 20% qual 50% qual 80% qual    AUC   APGR\n",
      "0  無進行資料採樣   19.92%   38.18%   72.35%   75.4  0.576\n",
      "1   進行資料採樣   19.63%   37.29%   71.92%  75.53  0.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7320/1939658251.py:53: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  return np.trapz(\n",
      "/tmp/ipykernel_7320/1939658251.py:64: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  weak_auc = np.trapz(weak_auc, df_per_method[\"strong_percentage\"] / 100)\n",
      "/tmp/ipykernel_7320/1939658251.py:68: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  strong_auc = np.trapz(strong_auc, df_per_method[\"strong_percentage\"] / 100)\n"
     ]
    }
   ],
   "source": [
    "# python -m routellm.evals.evaluate --routers random sw_ranking bert --benchmark mmlu --config config.example.yaml \n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import yaml\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "from routellm.controller import Controller\n",
    "from routellm.evals.benchmarks import GSM8K, MMLU, MTBench\n",
    "from routellm.evals.mmlu.domains import ALL_MMLU_DOMAINS\n",
    "from routellm.routers.routers import ROUTER_CLS\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "\n",
    "# 忽略 FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def generate_results(\n",
    "    df_router_result, benchmark, benchmark_name, routed_pair, output, plot_optimal=False\n",
    "):\n",
    " \n",
    "    weak_accuracy = benchmark.get_model_accuracy(routed_pair.weak)\n",
    "    print(f\"{routed_pair.weak} score: {weak_accuracy}\")\n",
    "\n",
    "    strong_accuracy = benchmark.get_model_accuracy(routed_pair.strong)\n",
    "    print(f\"{routed_pair.strong} score: {strong_accuracy}\")\n",
    "\n",
    "    def pct_call_metric(row):\n",
    "        df_per_method = df_router_result[\n",
    "            df_router_result[\"method\"] == row[\"method\"]\n",
    "        ].sort_values(by=[\"strong_percentage\"])\n",
    "        pct_calls = []\n",
    "\n",
    "        for pct in [0.2, 0.5, 0.8]:\n",
    "            pct_call = np.interp(\n",
    "                pct * (strong_accuracy - weak_accuracy) + weak_accuracy,\n",
    "                df_per_method[\"accuracy\"],\n",
    "                df_per_method[\"strong_percentage\"],\n",
    "            )\n",
    "            pct_calls.append(f\"{pct_call:.2f}%\")\n",
    "\n",
    "        return pd.Series(pct_calls)\n",
    "\n",
    "    def auc_metric(row):\n",
    "        df_per_method = df_router_result[\n",
    "            df_router_result[\"method\"] == row[\"method\"]\n",
    "        ].sort_values(by=[\"strong_percentage\"])\n",
    "        return np.trapz(\n",
    "            df_per_method[\"accuracy\"], df_per_method[\"strong_percentage\"] / 100\n",
    "        )\n",
    "\n",
    "    def apgr_metric(row):\n",
    "        df_per_method = df_router_result[\n",
    "            df_router_result[\"method\"] == row[\"method\"]\n",
    "        ].sort_values(by=[\"strong_percentage\"])\n",
    "\n",
    "        weak_auc = np.zeros([len(df_per_method)], dtype=float)\n",
    "        weak_auc.fill(weak_accuracy)\n",
    "        weak_auc = np.trapz(weak_auc, df_per_method[\"strong_percentage\"] / 100)\n",
    "\n",
    "        strong_auc = np.zeros([len(df_per_method)], dtype=float)\n",
    "        strong_auc.fill(strong_accuracy)\n",
    "        strong_auc = np.trapz(strong_auc, df_per_method[\"strong_percentage\"] / 100)\n",
    "\n",
    "        return (row[\"AUC\"] - weak_auc) / (strong_auc - weak_auc)\n",
    "\n",
    "    metrics = pd.DataFrame({\"method\": df_router_result[\"method\"].unique()})\n",
    "    metrics[[\"20% qual\", \"50% qual\", \"80% qual\"]] = metrics.apply(\n",
    "        pct_call_metric, axis=1\n",
    "    )\n",
    "    metrics[\"AUC\"] = metrics.apply(auc_metric, axis=1)\n",
    "    metrics[\"APGR\"] = metrics.apply(apgr_metric, axis=1)\n",
    "    from src.Utils.merge import f4\n",
    "    metrics = f4(metrics)\n",
    "    # metrics = metrics.sort_values(by=[\"APGR\"], ascending=False)\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "        print(\"Metrics:\\n\")\n",
    "        print(metrics)\n",
    "\n",
    "\n",
    "def pretty_print_results(threshold, accuracy, model_counts, total):\n",
    "    header = (\n",
    "        \"=\" * 15\n",
    "        + f\" {router} with threshold {threshold} on {args.benchmark} \"\n",
    "        + \"=\" * 15\n",
    "    )\n",
    "    print(\"\\n\" + header)\n",
    "    print(\"Average accuracy: {:.3f}\".format(accuracy))\n",
    "    print(f\"Model counts: {', '.join([f'{k}: {v}' for k, v in model_counts.items()])}\")\n",
    "    print(\n",
    "        f\"Model %: {', '.join([f'{k}: {v / total * 100:.3f}%' for k, v in model_counts.items()])}\"\n",
    "    )\n",
    "    print(\"=\" * len(header) + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "    from types import SimpleNamespace\n",
    "\n",
    "    args = SimpleNamespace(\n",
    "        routers=[\"bert\", \"random\"],\n",
    "        benchmark=\"mmlu\",\n",
    "        output=\".\",\n",
    "        overwrite_cache=[],\n",
    "        parallel=psutil.cpu_count(logical=False),\n",
    "        strong_model=\"gpt-4-1106-preview\",\n",
    "        weak_model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "        config='config.example.yaml',\n",
    "        num_results=10,\n",
    "        random_iters=10\n",
    "    )\n",
    "    # print(args)\n",
    "\n",
    "    pandarallel.initialize(progress_bar=True, nb_workers=args.parallel)\n",
    "    controller = Controller(\n",
    "        routers=args.routers,\n",
    "        config=yaml.safe_load(open(args.config, \"r\")) if args.config else None,\n",
    "        strong_model=args.strong_model,\n",
    "        weak_model=args.weak_model,\n",
    "        progress_bar=False,\n",
    "    )\n",
    "\n",
    "    if args.benchmark == \"mmlu\":\n",
    "        print(\"Running eval for full MMLU.\")\n",
    "        mmlu_domains = ALL_MMLU_DOMAINS\n",
    "        benchmark = MMLU(mmlu_domains, controller.model_pair, args.overwrite_cache)\n",
    "    elif args.benchmark == \"mt-bench\":\n",
    "        print(\"Running eval for MT Bench.\")\n",
    "        benchmark = MTBench(controller.model_pair, args.overwrite_cache)\n",
    "    elif args.benchmark == \"gsm8k\":\n",
    "        print(\"Running eval for GSM8k.\")\n",
    "        benchmark = GSM8K(controller.model_pair, args.overwrite_cache)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid benchmark {args.benchmark}\")\n",
    "\n",
    "    all_results = pd.DataFrame()\n",
    "    for router in controller.routers:\n",
    "        # Ensure reproducibility on a per-router basis\n",
    "        random.seed(0)\n",
    "        # For non-deterministic routers like random, we average over multiple runs\n",
    "        if router in [\"random\"]:\n",
    "            router_results = []\n",
    "            for i in range(args.random_iters):\n",
    "                for threshold, accuracy, model_counts, total in benchmark.evaluate(\n",
    "                    controller, router, args.num_results, True\n",
    "                ):\n",
    "                    router_results.append(\n",
    "                        {\n",
    "                            \"threshold\": threshold,\n",
    "                            \"strong_percentage\": model_counts[\n",
    "                                controller.model_pair.strong\n",
    "                            ]\n",
    "                            / total\n",
    "                            * 100,\n",
    "                            \"accuracy\": accuracy,\n",
    "                        }\n",
    "                    )\n",
    "            router_results_df = (\n",
    "                pd.DataFrame(router_results)\n",
    "                .groupby([\"strong_percentage\"], as_index=False)\n",
    "                .mean()\n",
    "            )\n",
    "            router_results_df[\"method\"] = str(router)\n",
    "            all_results = pd.concat([all_results, router_results_df])\n",
    "        else:\n",
    "            router_results = []\n",
    "            for threshold, accuracy, model_counts, total in benchmark.evaluate(\n",
    "                controller, router, args.num_results, False\n",
    "            ):\n",
    "                # print(f\"Evaluating router: {router} with threshold {threshold}...\")\n",
    "                # pretty_print_results(threshold, accuracy, model_counts, total)\n",
    "\n",
    "                result = {\n",
    "                    \"method\": str(router),\n",
    "                    \"threshold\": threshold,\n",
    "                    \"strong_percentage\": model_counts[controller.model_pair.strong]\n",
    "                    / total\n",
    "                    * 100,\n",
    "                    \"accuracy\": accuracy,\n",
    "                }\n",
    "                router_results.append(result)\n",
    "            all_results = pd.concat([all_results, pd.DataFrame(router_results)])\n",
    "\n",
    "    generate_results(\n",
    "        all_results,\n",
    "        benchmark,\n",
    "        args.benchmark,\n",
    "        controller.model_pair,\n",
    "        args.output,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9a59a1-9a00-410c-9564-561fd7f71a71",
   "metadata": {},
   "source": [
    "## 5.語義句法特徵提取與否實驗對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e280dc0-9a58-46e8-8703-81c992891ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 96 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/RouteLLM/pretrained/bert_gpt4_augmented\n",
      "Running eval for full MMLU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading domain data: 100%|██████████| 57/57 [00:00<00:00, 370.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining 14037/14042 prompts for MMLU after decontamination\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1 score: 68.09147253686685\n",
      "gpt-4-1106-preview score: 80.58702001852248\n",
      "Metrics:\n",
      "\n",
      "        method 20% qual 50% qual 80% qual    AUC   APGR\n",
      "0  無進行語義句法特徵提取   19.84%   38.72%   72.68%  75.31  0.570\n",
      "1   進行語義句法特徵提取   19.63%   37.29%   71.92%  75.53  0.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7320/821194505.py:53: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  return np.trapz(\n",
      "/tmp/ipykernel_7320/821194505.py:64: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  weak_auc = np.trapz(weak_auc, df_per_method[\"strong_percentage\"] / 100)\n",
      "/tmp/ipykernel_7320/821194505.py:68: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  strong_auc = np.trapz(strong_auc, df_per_method[\"strong_percentage\"] / 100)\n"
     ]
    }
   ],
   "source": [
    "# python -m routellm.evals.evaluate --routers random sw_ranking bert --benchmark mmlu --config config.example.yaml \n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import yaml\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "from routellm.controller import Controller\n",
    "from routellm.evals.benchmarks import GSM8K, MMLU, MTBench\n",
    "from routellm.evals.mmlu.domains import ALL_MMLU_DOMAINS\n",
    "from routellm.routers.routers import ROUTER_CLS\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import warnings\n",
    "\n",
    "# 忽略 FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "def generate_results(\n",
    "    df_router_result, benchmark, benchmark_name, routed_pair, output, plot_optimal=False\n",
    "):\n",
    " \n",
    "    weak_accuracy = benchmark.get_model_accuracy(routed_pair.weak)\n",
    "    print(f\"{routed_pair.weak} score: {weak_accuracy}\")\n",
    "\n",
    "    strong_accuracy = benchmark.get_model_accuracy(routed_pair.strong)\n",
    "    print(f\"{routed_pair.strong} score: {strong_accuracy}\")\n",
    "\n",
    "    def pct_call_metric(row):\n",
    "        df_per_method = df_router_result[\n",
    "            df_router_result[\"method\"] == row[\"method\"]\n",
    "        ].sort_values(by=[\"strong_percentage\"])\n",
    "        pct_calls = []\n",
    "\n",
    "        for pct in [0.2, 0.5, 0.8]:\n",
    "            pct_call = np.interp(\n",
    "                pct * (strong_accuracy - weak_accuracy) + weak_accuracy,\n",
    "                df_per_method[\"accuracy\"],\n",
    "                df_per_method[\"strong_percentage\"],\n",
    "            )\n",
    "            pct_calls.append(f\"{pct_call:.2f}%\")\n",
    "\n",
    "        return pd.Series(pct_calls)\n",
    "\n",
    "    def auc_metric(row):\n",
    "        df_per_method = df_router_result[\n",
    "            df_router_result[\"method\"] == row[\"method\"]\n",
    "        ].sort_values(by=[\"strong_percentage\"])\n",
    "        return np.trapz(\n",
    "            df_per_method[\"accuracy\"], df_per_method[\"strong_percentage\"] / 100\n",
    "        )\n",
    "\n",
    "    def apgr_metric(row):\n",
    "        df_per_method = df_router_result[\n",
    "            df_router_result[\"method\"] == row[\"method\"]\n",
    "        ].sort_values(by=[\"strong_percentage\"])\n",
    "\n",
    "        weak_auc = np.zeros([len(df_per_method)], dtype=float)\n",
    "        weak_auc.fill(weak_accuracy)\n",
    "        weak_auc = np.trapz(weak_auc, df_per_method[\"strong_percentage\"] / 100)\n",
    "\n",
    "        strong_auc = np.zeros([len(df_per_method)], dtype=float)\n",
    "        strong_auc.fill(strong_accuracy)\n",
    "        strong_auc = np.trapz(strong_auc, df_per_method[\"strong_percentage\"] / 100)\n",
    "\n",
    "        return (row[\"AUC\"] - weak_auc) / (strong_auc - weak_auc)\n",
    "\n",
    "    metrics = pd.DataFrame({\"method\": df_router_result[\"method\"].unique()})\n",
    "    metrics[[\"20% qual\", \"50% qual\", \"80% qual\"]] = metrics.apply(\n",
    "        pct_call_metric, axis=1\n",
    "    )\n",
    "    metrics[\"AUC\"] = metrics.apply(auc_metric, axis=1)\n",
    "    metrics[\"APGR\"] = metrics.apply(apgr_metric, axis=1)\n",
    "    from src.Utils.merge import f5\n",
    "    metrics = f5(metrics)\n",
    "    # metrics = metrics.sort_values(by=[\"APGR\"], ascending=False)\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "        print(\"Metrics:\\n\")\n",
    "        print(metrics)\n",
    "\n",
    "\n",
    "def pretty_print_results(threshold, accuracy, model_counts, total):\n",
    "    header = (\n",
    "        \"=\" * 15\n",
    "        + f\" {router} with threshold {threshold} on {args.benchmark} \"\n",
    "        + \"=\" * 15\n",
    "    )\n",
    "    print(\"\\n\" + header)\n",
    "    print(\"Average accuracy: {:.3f}\".format(accuracy))\n",
    "    print(f\"Model counts: {', '.join([f'{k}: {v}' for k, v in model_counts.items()])}\")\n",
    "    print(\n",
    "        f\"Model %: {', '.join([f'{k}: {v / total * 100:.3f}%' for k, v in model_counts.items()])}\"\n",
    "    )\n",
    "    print(\"=\" * len(header) + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "    from types import SimpleNamespace\n",
    "\n",
    "    args = SimpleNamespace(\n",
    "        routers=[\"bert\", \"random\"],\n",
    "        benchmark=\"mmlu\",\n",
    "        output=\".\",\n",
    "        overwrite_cache=[],\n",
    "        parallel=psutil.cpu_count(logical=False),\n",
    "        strong_model=\"gpt-4-1106-preview\",\n",
    "        weak_model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "        config='config.example.yaml',\n",
    "        num_results=10,\n",
    "        random_iters=10\n",
    "    )\n",
    "    # print(args)\n",
    "\n",
    "    pandarallel.initialize(progress_bar=True, nb_workers=args.parallel)\n",
    "    controller = Controller(\n",
    "        routers=args.routers,\n",
    "        config=yaml.safe_load(open(args.config, \"r\")) if args.config else None,\n",
    "        strong_model=args.strong_model,\n",
    "        weak_model=args.weak_model,\n",
    "        progress_bar=False,\n",
    "    )\n",
    "\n",
    "    if args.benchmark == \"mmlu\":\n",
    "        print(\"Running eval for full MMLU.\")\n",
    "        mmlu_domains = ALL_MMLU_DOMAINS\n",
    "        benchmark = MMLU(mmlu_domains, controller.model_pair, args.overwrite_cache)\n",
    "    elif args.benchmark == \"mt-bench\":\n",
    "        print(\"Running eval for MT Bench.\")\n",
    "        benchmark = MTBench(controller.model_pair, args.overwrite_cache)\n",
    "    elif args.benchmark == \"gsm8k\":\n",
    "        print(\"Running eval for GSM8k.\")\n",
    "        benchmark = GSM8K(controller.model_pair, args.overwrite_cache)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid benchmark {args.benchmark}\")\n",
    "\n",
    "    all_results = pd.DataFrame()\n",
    "    for router in controller.routers:\n",
    "        # Ensure reproducibility on a per-router basis\n",
    "        random.seed(0)\n",
    "        # For non-deterministic routers like random, we average over multiple runs\n",
    "        if router in [\"random\"]:\n",
    "            router_results = []\n",
    "            for i in range(args.random_iters):\n",
    "                for threshold, accuracy, model_counts, total in benchmark.evaluate(\n",
    "                    controller, router, args.num_results, True\n",
    "                ):\n",
    "                    router_results.append(\n",
    "                        {\n",
    "                            \"threshold\": threshold,\n",
    "                            \"strong_percentage\": model_counts[\n",
    "                                controller.model_pair.strong\n",
    "                            ]\n",
    "                            / total\n",
    "                            * 100,\n",
    "                            \"accuracy\": accuracy,\n",
    "                        }\n",
    "                    )\n",
    "            router_results_df = (\n",
    "                pd.DataFrame(router_results)\n",
    "                .groupby([\"strong_percentage\"], as_index=False)\n",
    "                .mean()\n",
    "            )\n",
    "            router_results_df[\"method\"] = str(router)\n",
    "            all_results = pd.concat([all_results, router_results_df])\n",
    "        else:\n",
    "            router_results = []\n",
    "            for threshold, accuracy, model_counts, total in benchmark.evaluate(\n",
    "                controller, router, args.num_results, False\n",
    "            ):\n",
    "                # print(f\"Evaluating router: {router} with threshold {threshold}...\")\n",
    "                # pretty_print_results(threshold, accuracy, model_counts, total)\n",
    "\n",
    "                result = {\n",
    "                    \"method\": str(router),\n",
    "                    \"threshold\": threshold,\n",
    "                    \"strong_percentage\": model_counts[controller.model_pair.strong]\n",
    "                    / total\n",
    "                    * 100,\n",
    "                    \"accuracy\": accuracy,\n",
    "                }\n",
    "                router_results.append(result)\n",
    "            all_results = pd.concat([all_results, pd.DataFrame(router_results)])\n",
    "\n",
    "    generate_results(\n",
    "        all_results,\n",
    "        benchmark,\n",
    "        args.benchmark,\n",
    "        controller.model_pair,\n",
    "        args.output,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f807dc-2896-4982-b7eb-21c0d59612e0",
   "metadata": {},
   "source": [
    "## 6.搭建 RAG 框架與否實驗對比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9559c6b6-3b79-4851-8d3f-352cd0f1b5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 96 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/RouteLLM/pretrained/bert_gpt4_augmented\n",
      "Running eval for full MMLU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading domain data: 100%|██████████| 57/57 [00:00<00:00, 629.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining 14037/14042 prompts for MMLU after decontamination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/Mixtral-8x7B-Instruct-v0.1 score: 68.09147253686685\n",
      "gpt-4-1106-preview score: 80.58702001852248\n",
      "Metrics:\n",
      "\n",
      "          method 20% qual 50% qual 80% qual    AUC   APGR\n",
      "0  無搭建 RAG 向量知識庫   19.81%   39.65%   73.74%  75.15  0.557\n",
      "1   搭建 RAG 向量知識庫   19.63%   37.29%   71.92%  75.53  0.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7488/3571735034.py:53: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  return np.trapz(\n",
      "/tmp/ipykernel_7488/3571735034.py:64: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  weak_auc = np.trapz(weak_auc, df_per_method[\"strong_percentage\"] / 100)\n",
      "/tmp/ipykernel_7488/3571735034.py:68: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  strong_auc = np.trapz(strong_auc, df_per_method[\"strong_percentage\"] / 100)\n"
     ]
    }
   ],
   "source": [
    "# python -m routellm.evals.evaluate --routers random sw_ranking bert --benchmark mmlu --config config.example.yaml \n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import yaml\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "from routellm.controller import Controller\n",
    "from routellm.evals.benchmarks import GSM8K, MMLU, MTBench\n",
    "from routellm.evals.mmlu.domains import ALL_MMLU_DOMAINS\n",
    "from routellm.routers.routers import ROUTER_CLS\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import warnings\n",
    "\n",
    "# 忽略 FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def generate_results(\n",
    "    df_router_result, benchmark, benchmark_name, routed_pair, output, plot_optimal=False\n",
    "):\n",
    " \n",
    "    weak_accuracy = benchmark.get_model_accuracy(routed_pair.weak)\n",
    "    print(f\"{routed_pair.weak} score: {weak_accuracy}\")\n",
    "\n",
    "    strong_accuracy = benchmark.get_model_accuracy(routed_pair.strong)\n",
    "    print(f\"{routed_pair.strong} score: {strong_accuracy}\")\n",
    "\n",
    "    def pct_call_metric(row):\n",
    "        df_per_method = df_router_result[\n",
    "            df_router_result[\"method\"] == row[\"method\"]\n",
    "        ].sort_values(by=[\"strong_percentage\"])\n",
    "        pct_calls = []\n",
    "\n",
    "        for pct in [0.2, 0.5, 0.8]:\n",
    "            pct_call = np.interp(\n",
    "                pct * (strong_accuracy - weak_accuracy) + weak_accuracy,\n",
    "                df_per_method[\"accuracy\"],\n",
    "                df_per_method[\"strong_percentage\"],\n",
    "            )\n",
    "            pct_calls.append(f\"{pct_call:.2f}%\")\n",
    "\n",
    "        return pd.Series(pct_calls)\n",
    "\n",
    "    def auc_metric(row):\n",
    "        df_per_method = df_router_result[\n",
    "            df_router_result[\"method\"] == row[\"method\"]\n",
    "        ].sort_values(by=[\"strong_percentage\"])\n",
    "        return np.trapz(\n",
    "            df_per_method[\"accuracy\"], df_per_method[\"strong_percentage\"] / 100\n",
    "        )\n",
    "\n",
    "    def apgr_metric(row):\n",
    "        df_per_method = df_router_result[\n",
    "            df_router_result[\"method\"] == row[\"method\"]\n",
    "        ].sort_values(by=[\"strong_percentage\"])\n",
    "\n",
    "        weak_auc = np.zeros([len(df_per_method)], dtype=float)\n",
    "        weak_auc.fill(weak_accuracy)\n",
    "        weak_auc = np.trapz(weak_auc, df_per_method[\"strong_percentage\"] / 100)\n",
    "\n",
    "        strong_auc = np.zeros([len(df_per_method)], dtype=float)\n",
    "        strong_auc.fill(strong_accuracy)\n",
    "        strong_auc = np.trapz(strong_auc, df_per_method[\"strong_percentage\"] / 100)\n",
    "\n",
    "        return (row[\"AUC\"] - weak_auc) / (strong_auc - weak_auc)\n",
    "\n",
    "    metrics = pd.DataFrame({\"method\": df_router_result[\"method\"].unique()})\n",
    "    metrics[[\"20% qual\", \"50% qual\", \"80% qual\"]] = metrics.apply(\n",
    "        pct_call_metric, axis=1\n",
    "    )\n",
    "    metrics[\"AUC\"] = metrics.apply(auc_metric, axis=1)\n",
    "    metrics[\"APGR\"] = metrics.apply(apgr_metric, axis=1)\n",
    "    from src.Utils.merge import f6\n",
    "    metrics = f6(metrics)\n",
    "    # metrics = metrics.sort_values(by=[\"APGR\"], ascending=False)\n",
    "    with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "        print(\"Metrics:\\n\")\n",
    "        print(metrics)\n",
    "\n",
    "\n",
    "def pretty_print_results(threshold, accuracy, model_counts, total):\n",
    "    header = (\n",
    "        \"=\" * 15\n",
    "        + f\" {router} with threshold {threshold} on {args.benchmark} \"\n",
    "        + \"=\" * 15\n",
    "    )\n",
    "    print(\"\\n\" + header)\n",
    "    print(\"Average accuracy: {:.3f}\".format(accuracy))\n",
    "    print(f\"Model counts: {', '.join([f'{k}: {v}' for k, v in model_counts.items()])}\")\n",
    "    print(\n",
    "        f\"Model %: {', '.join([f'{k}: {v / total * 100:.3f}%' for k, v in model_counts.items()])}\"\n",
    "    )\n",
    "    print(\"=\" * len(header) + \"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "    from types import SimpleNamespace\n",
    "\n",
    "    args = SimpleNamespace(\n",
    "        routers=[\"bert\", \"random\"],\n",
    "        benchmark=\"mmlu\",\n",
    "        output=\".\",\n",
    "        overwrite_cache=[],\n",
    "        parallel=psutil.cpu_count(logical=False),\n",
    "        strong_model=\"gpt-4-1106-preview\",\n",
    "        weak_model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "        config='config.example.yaml',\n",
    "        num_results=10,\n",
    "        random_iters=10\n",
    "    )\n",
    "    # print(args)\n",
    "\n",
    "    pandarallel.initialize(progress_bar=True, nb_workers=args.parallel)\n",
    "    controller = Controller(\n",
    "        routers=args.routers,\n",
    "        config=yaml.safe_load(open(args.config, \"r\")) if args.config else None,\n",
    "        strong_model=args.strong_model,\n",
    "        weak_model=args.weak_model,\n",
    "        progress_bar=False,\n",
    "    )\n",
    "\n",
    "    if args.benchmark == \"mmlu\":\n",
    "        print(\"Running eval for full MMLU.\")\n",
    "        mmlu_domains = ALL_MMLU_DOMAINS\n",
    "        benchmark = MMLU(mmlu_domains, controller.model_pair, args.overwrite_cache)\n",
    "    elif args.benchmark == \"mt-bench\":\n",
    "        print(\"Running eval for MT Bench.\")\n",
    "        benchmark = MTBench(controller.model_pair, args.overwrite_cache)\n",
    "    elif args.benchmark == \"gsm8k\":\n",
    "        print(\"Running eval for GSM8k.\")\n",
    "        benchmark = GSM8K(controller.model_pair, args.overwrite_cache)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid benchmark {args.benchmark}\")\n",
    "\n",
    "    all_results = pd.DataFrame()\n",
    "    for router in controller.routers:\n",
    "        # Ensure reproducibility on a per-router basis\n",
    "        random.seed(0)\n",
    "        # For non-deterministic routers like random, we average over multiple runs\n",
    "        if router in [\"random\"]:\n",
    "            router_results = []\n",
    "            for i in range(args.random_iters):\n",
    "                for threshold, accuracy, model_counts, total in benchmark.evaluate(\n",
    "                    controller, router, args.num_results, True\n",
    "                ):\n",
    "                    router_results.append(\n",
    "                        {\n",
    "                            \"threshold\": threshold,\n",
    "                            \"strong_percentage\": model_counts[\n",
    "                                controller.model_pair.strong\n",
    "                            ]\n",
    "                            / total\n",
    "                            * 100,\n",
    "                            \"accuracy\": accuracy,\n",
    "                        }\n",
    "                    )\n",
    "            router_results_df = (\n",
    "                pd.DataFrame(router_results)\n",
    "                .groupby([\"strong_percentage\"], as_index=False)\n",
    "                .mean()\n",
    "            )\n",
    "            router_results_df[\"method\"] = str(router)\n",
    "            all_results = pd.concat([all_results, router_results_df])\n",
    "        else:\n",
    "            router_results = []\n",
    "            for threshold, accuracy, model_counts, total in benchmark.evaluate(\n",
    "                controller, router, args.num_results, False\n",
    "            ):\n",
    "                # print(f\"Evaluating router: {router} with threshold {threshold}...\")\n",
    "                # pretty_print_results(threshold, accuracy, model_counts, total)\n",
    "\n",
    "                result = {\n",
    "                    \"method\": str(router),\n",
    "                    \"threshold\": threshold,\n",
    "                    \"strong_percentage\": model_counts[controller.model_pair.strong]\n",
    "                    / total\n",
    "                    * 100,\n",
    "                    \"accuracy\": accuracy,\n",
    "                }\n",
    "                router_results.append(result)\n",
    "            all_results = pd.concat([all_results, pd.DataFrame(router_results)])\n",
    "\n",
    "    generate_results(\n",
    "        all_results,\n",
    "        benchmark,\n",
    "        args.benchmark,\n",
    "        controller.model_pair,\n",
    "        args.output,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d48e1fb-31d3-47ea-b0db-a765fa4d1ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "route",
   "language": "python",
   "name": "route"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
